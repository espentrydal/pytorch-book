{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad33fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "# Use smaller model if you have limited resources\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side=\"left\")\n",
    "\n",
    "def get_serialized_model_size_in_mb(model):\n",
    "    \"\"\"Calculate the size of the serialized model in megabytes.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    torch.save(model, buffer)\n",
    "    size_in_bytes = buffer.tell()\n",
    "    buffer.close()\n",
    "    size_in_mb = size_in_bytes / (1024**2)  # Convert bytes to megabytes\n",
    "    return size_in_mb\n",
    "\n",
    "def run_model(model):\n",
    "    text = \"Tell me a fun fact in a short sentence.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_length = input_ids.shape[1]\n",
    "    output_ids = model.generate(input_ids, max_length=200, do_sample=True)[0]\n",
    "    response_ids = output_ids[input_length:]\n",
    "    return tokenizer.decode(response_ids, ignore_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "339a967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized model size before quantization: 6528.52 MB\n",
      "<|im_start|>assistant\n",
      "Did you know ancient Egyptians often buried food alongside their mummies to ensure a full belly in the afterlife?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "size_before_quantization = get_serialized_model_size_in_mb(model)\n",
    "print(f\"Serialized model size before quantization: {size_before_quantization:.2f} MB\")\n",
    "print(run_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd698527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized model size after quantization1: 3264.33 MB\n",
      "<|im_start|>assistant\n",
      "Did you know that penguins can only fly underwater, not in the sky like humans?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# This quantizes the weights from bfloat32 to bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model = model.to(torch.bfloat16)\n",
    "size_after_quantization = get_serialized_model_size_in_mb(model)\n",
    "print(f\"Serialized model size after quantization1: {size_after_quantization:.2f} MB\")\n",
    "print(run_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846caf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized model size after quantization2: 1825.69 MB\n",
      "<|im_start|>assistant\n",
      "Did you know that the longest-living creature in the animal kingdom can grow up to 21ft long and has the average lifespan of 100 years?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def int8_symmetric_quantize(\n",
    "    fp32_tensor: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    quant_min = -128\n",
    "    quant_max = 127\n",
    "    min_val = torch.amin(fp32_tensor, dim=[1], keepdim=False)\n",
    "    max_val = torch.amax(fp32_tensor, dim=[1], keepdim=False)\n",
    "    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n",
    "    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n",
    "    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n",
    "    scale = max_val_pos / (float(quant_max - quant_min) / 2)\n",
    "    scale = scale.view(fp32_tensor.shape[0], -1)\n",
    "    out = torch.round(fp32_tensor * (1.0 / scale))\n",
    "    out = torch.clamp(out, quant_min, quant_max).to(torch.int8)\n",
    "    return out, scale\n",
    "\n",
    "def quantize_linear_layers_to_int8(model):\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            weight = module.weight.data\n",
    "            w_int8, scale = int8_symmetric_quantize(weight)\n",
    "            \n",
    "            # Store the int8 weights and scale directly in the module\n",
    "            module.register_buffer('weight_int8', w_int8)\n",
    "            module.register_buffer('weight_scale', scale)\n",
    "            # Remove the original weight to save space\n",
    "            delattr(module, 'weight')\n",
    "            \n",
    "            # Create a new forward method that uses the quantized weights\n",
    "            def new_forward(self, x):\n",
    "                # Dequantize during inference\n",
    "                dequantized_weight = self.weight_int8.to(x.dtype) * self.weight_scale\n",
    "                return torch.nn.functional.linear(x, dequantized_weight, self.bias)\n",
    "            import types\n",
    "            module.forward = types.MethodType(new_forward, module)\n",
    "    return model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "model = model.to(torch.bfloat16)\n",
    "model = quantize_linear_layers_to_int8(model)\n",
    "size_after_quantization2 = get_serialized_model_size_in_mb(model)\n",
    "print(f\"Serialized model size after quantization2: {size_after_quantization2:.2f} MB\")\n",
    "print(run_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3a4348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 6528.52 MB\n",
      "Quantized size: 2016.57 MB\n",
      "Size reduction: 69.11%\n",
      "assistant A Your I system is what is it says. Do You want a question Your Your Question might be â€œIs Rome a great city for ladies If You ever get out of town in Italy for tourism or even business. yes the Romans I mean the ancient romans Are we talking about any city A city could be built near Rome if A city A city is What do you want to do You are doing The City Well The Romans built Rome A huge city city So you Can do whatever you want now In Rome Because the Roman government And The Romans Are Famous A whole for building stuff Rome The Romans built more then almost anything now Rome is it was What they called for Rome The ancient roman Rome is also called The Eternal Rome Which means The old Rome Is always Rome The Romans built Rome a very tall tower Which we\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# Apply dynamic quantization - targets only the linear layers\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},  # Only quantize linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Check size and run inference\n",
    "size_before = get_serialized_model_size_in_mb(model)\n",
    "size_after = get_serialized_model_size_in_mb(quantized_model)\n",
    "print(f\"Original size: {size_before:.2f} MB\")\n",
    "print(f\"Quantized size: {size_after:.2f} MB\")\n",
    "print(f\"Size reduction: {100 * (1 - size_after / size_before):.2f}%\")\n",
    "print(run_model(quantized_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffae755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2], dtype=torch.int8)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"softmax_lastdim_kernel_impl\" not implemented for 'Char'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m x_int8 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint8)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_int8)  \u001b[38;5;66;03m# [0, 2, 3] - already losing precision\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_int8 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_int8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Error: sigmoid not implemented for 'Int8'\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"softmax_lastdim_kernel_impl\" not implemented for 'Char'"
     ]
    }
   ],
   "source": [
    "# This works in floating point\n",
    "x = torch.tensor([0.5, 1.5, 2.5])\n",
    "\n",
    "x_int8 = x.to(torch.int8)\n",
    "print(x_int8)  # [0, 2, 3] - already losing precision\n",
    "y_int8 = torch.softmax(x_int8, dim=0)  # Error: softmax not implemented for 'Int8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccf49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
