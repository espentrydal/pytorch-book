{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24414034bb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                3072,  # <1>\n",
    "                512,   # <2>\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(\n",
    "                512,   # <2>\n",
    "                n_out, # <3>\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.6652, 0.2447, 0.0900]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [3.0, 2.0, 1.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8624594..2.0298588].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)\n",
    "img_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5079, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.832332\n",
      "Epoch: 1, Loss: 1.627654\n",
      "Epoch: 2, Loss: 7.493120\n",
      "Epoch: 3, Loss: 8.217752\n",
      "Epoch: 4, Loss: 3.293499\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        img_tensor = img.view(-1).unsqueeze(0)\n",
    "        label_tensor = torch.tensor([label])\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howardhuang\\AppData\\Local\\Temp\\ipykernel_31152\\1182167889.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.318584\n",
      "Epoch: 1, Loss: 0.389656\n",
      "Epoch: 2, Loss: 0.428063\n",
      "Epoch: 3, Loss: 0.435959\n",
      "Epoch: 4, Loss: 0.504194\n",
      "Epoch: 5, Loss: 0.483706\n",
      "Epoch: 6, Loss: 0.308477\n",
      "Epoch: 7, Loss: 0.115593\n",
      "Epoch: 8, Loss: 0.513745\n",
      "Epoch: 9, Loss: 0.386390\n",
      "Epoch: 10, Loss: 0.275170\n",
      "Epoch: 11, Loss: 0.252209\n",
      "Epoch: 12, Loss: 0.213220\n",
      "Epoch: 13, Loss: 0.256754\n",
      "Epoch: 14, Loss: 0.317098\n",
      "Epoch: 15, Loss: 0.631423\n",
      "Epoch: 16, Loss: 0.423116\n",
      "Epoch: 17, Loss: 0.403038\n",
      "Epoch: 18, Loss: 0.307520\n",
      "Epoch: 19, Loss: 0.179391\n",
      "Epoch: 20, Loss: 0.379253\n",
      "Epoch: 21, Loss: 0.290552\n",
      "Epoch: 22, Loss: 0.270082\n",
      "Epoch: 23, Loss: 0.186669\n",
      "Epoch: 24, Loss: 0.284896\n",
      "Epoch: 25, Loss: 0.456809\n",
      "Epoch: 26, Loss: 0.262925\n",
      "Epoch: 27, Loss: 0.257303\n",
      "Epoch: 28, Loss: 0.253179\n",
      "Epoch: 29, Loss: 0.087952\n",
      "Epoch: 30, Loss: 0.119812\n",
      "Epoch: 31, Loss: 0.101360\n",
      "Epoch: 32, Loss: 0.247133\n",
      "Epoch: 33, Loss: 0.091199\n",
      "Epoch: 34, Loss: 0.172338\n",
      "Epoch: 35, Loss: 0.462693\n",
      "Epoch: 36, Loss: 0.273261\n",
      "Epoch: 37, Loss: 0.254027\n",
      "Epoch: 38, Loss: 0.349431\n",
      "Epoch: 39, Loss: 0.300603\n",
      "Epoch: 40, Loss: 0.063583\n",
      "Epoch: 41, Loss: 0.079538\n",
      "Epoch: 42, Loss: 0.135730\n",
      "Epoch: 43, Loss: 0.100508\n",
      "Epoch: 44, Loss: 0.055167\n",
      "Epoch: 45, Loss: 0.177686\n",
      "Epoch: 46, Loss: 0.147216\n",
      "Epoch: 47, Loss: 0.030162\n",
      "Epoch: 48, Loss: 0.113541\n",
      "Epoch: 49, Loss: 0.054586\n",
      "Epoch: 50, Loss: 0.088352\n",
      "Epoch: 51, Loss: 0.037682\n",
      "Epoch: 52, Loss: 0.044880\n",
      "Epoch: 53, Loss: 0.045166\n",
      "Epoch: 54, Loss: 0.122623\n",
      "Epoch: 55, Loss: 0.094353\n",
      "Epoch: 56, Loss: 0.114291\n",
      "Epoch: 57, Loss: 0.021039\n",
      "Epoch: 58, Loss: 0.047415\n",
      "Epoch: 59, Loss: 0.055022\n",
      "Epoch: 60, Loss: 0.057732\n",
      "Epoch: 61, Loss: 0.045904\n",
      "Epoch: 62, Loss: 0.019066\n",
      "Epoch: 63, Loss: 0.017869\n",
      "Epoch: 64, Loss: 0.063006\n",
      "Epoch: 65, Loss: 0.024748\n",
      "Epoch: 66, Loss: 0.028052\n",
      "Epoch: 67, Loss: 0.061628\n",
      "Epoch: 68, Loss: 0.025470\n",
      "Epoch: 69, Loss: 0.021451\n",
      "Epoch: 70, Loss: 0.015868\n",
      "Epoch: 71, Loss: 0.024140\n",
      "Epoch: 72, Loss: 0.139661\n",
      "Epoch: 73, Loss: 0.025053\n",
      "Epoch: 74, Loss: 0.017782\n",
      "Epoch: 75, Loss: 0.012453\n",
      "Epoch: 76, Loss: 0.062699\n",
      "Epoch: 77, Loss: 0.019221\n",
      "Epoch: 78, Loss: 0.026774\n",
      "Epoch: 79, Loss: 0.022550\n",
      "Epoch: 80, Loss: 0.006021\n",
      "Epoch: 81, Loss: 0.012875\n",
      "Epoch: 82, Loss: 0.010667\n",
      "Epoch: 83, Loss: 0.002635\n",
      "Epoch: 84, Loss: 0.017264\n",
      "Epoch: 85, Loss: 0.010242\n",
      "Epoch: 86, Loss: 0.009464\n",
      "Epoch: 87, Loss: 0.034903\n",
      "Epoch: 88, Loss: 0.013490\n",
      "Epoch: 89, Loss: 0.009381\n",
      "Epoch: 90, Loss: 0.010807\n",
      "Epoch: 91, Loss: 0.015975\n",
      "Epoch: 92, Loss: 0.011527\n",
      "Epoch: 93, Loss: 0.019438\n",
      "Epoch: 94, Loss: 0.011856\n",
      "Epoch: 95, Loss: 0.025044\n",
      "Epoch: 96, Loss: 0.015265\n",
      "Epoch: 97, Loss: 0.008149\n",
      "Epoch: 98, Loss: 0.011302\n",
      "Epoch: 99, Loss: 0.013323\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        img_tensor = imgs.view(batch_size, -1)\n",
    "        label_tensor = torch.tensor(labels)\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.811000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howardhuang\\AppData\\Local\\Temp\\ipykernel_31152\\3226406213.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.571252\n",
      "Epoch: 1, Loss: 0.313666\n",
      "Epoch: 2, Loss: 0.317773\n",
      "Epoch: 3, Loss: 0.339210\n",
      "Epoch: 4, Loss: 0.396160\n",
      "Epoch: 5, Loss: 0.502180\n",
      "Epoch: 6, Loss: 0.272780\n",
      "Epoch: 7, Loss: 0.275592\n",
      "Epoch: 8, Loss: 0.294269\n",
      "Epoch: 9, Loss: 0.277061\n",
      "Epoch: 10, Loss: 0.588710\n",
      "Epoch: 11, Loss: 0.679755\n",
      "Epoch: 12, Loss: 0.695176\n",
      "Epoch: 13, Loss: 0.216517\n",
      "Epoch: 14, Loss: 0.257015\n",
      "Epoch: 15, Loss: 0.367191\n",
      "Epoch: 16, Loss: 0.215245\n",
      "Epoch: 17, Loss: 0.159660\n",
      "Epoch: 18, Loss: 0.293674\n",
      "Epoch: 19, Loss: 0.328192\n",
      "Epoch: 20, Loss: 0.375341\n",
      "Epoch: 21, Loss: 0.381771\n",
      "Epoch: 22, Loss: 0.422056\n",
      "Epoch: 23, Loss: 0.185983\n",
      "Epoch: 24, Loss: 0.388639\n",
      "Epoch: 25, Loss: 0.295786\n",
      "Epoch: 26, Loss: 0.168499\n",
      "Epoch: 27, Loss: 0.273705\n",
      "Epoch: 28, Loss: 0.075720\n",
      "Epoch: 29, Loss: 0.225853\n",
      "Epoch: 30, Loss: 0.097164\n",
      "Epoch: 31, Loss: 0.135111\n",
      "Epoch: 32, Loss: 0.083771\n",
      "Epoch: 33, Loss: 0.213543\n",
      "Epoch: 34, Loss: 0.077632\n",
      "Epoch: 35, Loss: 0.293021\n",
      "Epoch: 36, Loss: 0.464636\n",
      "Epoch: 37, Loss: 0.069508\n",
      "Epoch: 38, Loss: 0.351075\n",
      "Epoch: 39, Loss: 0.042970\n",
      "Epoch: 40, Loss: 0.395650\n",
      "Epoch: 41, Loss: 0.090121\n",
      "Epoch: 42, Loss: 0.138659\n",
      "Epoch: 43, Loss: 0.041306\n",
      "Epoch: 44, Loss: 0.034770\n",
      "Epoch: 45, Loss: 0.157361\n",
      "Epoch: 46, Loss: 0.069251\n",
      "Epoch: 47, Loss: 0.207403\n",
      "Epoch: 48, Loss: 0.018327\n",
      "Epoch: 49, Loss: 0.045020\n",
      "Epoch: 50, Loss: 0.129736\n",
      "Epoch: 51, Loss: 0.097694\n",
      "Epoch: 52, Loss: 0.772088\n",
      "Epoch: 53, Loss: 0.015776\n",
      "Epoch: 54, Loss: 0.060927\n",
      "Epoch: 55, Loss: 0.012670\n",
      "Epoch: 56, Loss: 0.006956\n",
      "Epoch: 57, Loss: 0.013594\n",
      "Epoch: 58, Loss: 0.039618\n",
      "Epoch: 59, Loss: 0.007520\n",
      "Epoch: 60, Loss: 0.017369\n",
      "Epoch: 61, Loss: 0.006267\n",
      "Epoch: 62, Loss: 0.011182\n",
      "Epoch: 63, Loss: 0.007419\n",
      "Epoch: 64, Loss: 0.052596\n",
      "Epoch: 65, Loss: 0.053333\n",
      "Epoch: 66, Loss: 0.349586\n",
      "Epoch: 67, Loss: 0.014449\n",
      "Epoch: 68, Loss: 0.003369\n",
      "Epoch: 69, Loss: 0.006431\n",
      "Epoch: 70, Loss: 0.016606\n",
      "Epoch: 71, Loss: 0.001759\n",
      "Epoch: 72, Loss: 0.006898\n",
      "Epoch: 73, Loss: 0.002455\n",
      "Epoch: 74, Loss: 0.002453\n",
      "Epoch: 75, Loss: 0.006289\n",
      "Epoch: 76, Loss: 0.006173\n",
      "Epoch: 77, Loss: 0.003469\n",
      "Epoch: 78, Loss: 0.046140\n",
      "Epoch: 79, Loss: 0.006046\n",
      "Epoch: 80, Loss: 0.002480\n",
      "Epoch: 81, Loss: 0.002482\n",
      "Epoch: 82, Loss: 0.005167\n",
      "Epoch: 83, Loss: 0.000530\n",
      "Epoch: 84, Loss: 0.002561\n",
      "Epoch: 85, Loss: 0.000618\n",
      "Epoch: 86, Loss: 0.169858\n",
      "Epoch: 87, Loss: 0.008947\n",
      "Epoch: 88, Loss: 0.003302\n",
      "Epoch: 89, Loss: 0.003499\n",
      "Epoch: 90, Loss: 0.000741\n",
      "Epoch: 91, Loss: 0.001199\n",
      "Epoch: 92, Loss: 0.002039\n",
      "Epoch: 93, Loss: 0.006425\n",
      "Epoch: 94, Loss: 0.001258\n",
      "Epoch: 95, Loss: 0.000673\n",
      "Epoch: 96, Loss: 0.002405\n",
      "Epoch: 97, Loss: 0.002683\n",
      "Epoch: 98, Loss: 0.001703\n",
      "Epoch: 99, Loss: 0.001460\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        img_tensor = imgs.view(batch_size, -1)\n",
    "        label_tensor = torch.tensor(labels)\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.813000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574402, [1572864, 512, 1024, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
